{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataLoaders.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMqo4wUCZwh42NiuO142swI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ssvfTClBCPLa","executionInfo":{"status":"ok","timestamp":1604053616207,"user_tz":-60,"elapsed":25822,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}},"outputId":"74743ad4-71df-4eec-97a3-418316347283","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KlVaaig8Cw6t","executionInfo":{"status":"ok","timestamp":1604059425472,"user_tz":-60,"elapsed":768,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}}},"source":["import torch\n","import numpy as np\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torchvision\n","from torch.utils.data.dataset import Dataset, Subset\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import cv2\n","import copy\n","\n","\n","from torch._utils import _accumulate"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"juS9Jh7JC9Id","executionInfo":{"status":"ok","timestamp":1604057295623,"user_tz":-60,"elapsed":573,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}}},"source":["class ObjectsDataset(Dataset):\n","  \n","  def __init__(self, dataset_dir, frames, labels_index, labels_names, image_type):\n","    # image_type can be original, gray image or color-opponency. It is expected \n","    # that images with these types are already available - gotten through preprocessing offline. \n","    \n","    self.labels = labels_index\n","    self.dataset_dir = dataset_dir\n","    self.transforms = transforms\n","    self.image_type = image_type\n","    self.frames = frames\n","    self.labels_names = labels_names\n","    self.dataset_dir = dataset_dir\n","    \n","    \n","  def __len__(self):\n","    return len(self.frames)\n","    \n","  def __getitem__(self, index):\n","    frame_path = self.frames[index]\n","    label_index = self.labels[index]\n","    plant_name = self.labels_names[label_index]\n","    \n","    image = cv2.imread(frame_path)\n","    #image = cv2.resize(image, (1080, 720))  \n","    \n","    image = image/ 255.0\n","    image=torch.from_numpy(image.astype('float32')).permute(2, 0, 1)\n","    \n","    label=torch.from_numpy(np.asarray(label_index).astype('long'))\n","        \n","    return (image, label) "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZ50vrAmDPh6","executionInfo":{"status":"ok","timestamp":1604057839372,"user_tz":-60,"elapsed":633,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}}},"source":["import os\n","import re\n","\n","def natural_key(string_):\n","    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n","    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_)]\n","\n","def get_dataloader(image_type='original', skip=6, batch_size=64, shuffle=True):\n","    classes = ['bag','beer','book','case','coffee','cup','deodorant','eraser',\n","               'hole','mouse','mug','sleep','speaker','spray','stapler','tape',\n","               'tea','tissues','umbrella','watch']\n","    \n","    root_path = '/content/drive/My Drive/RODframes2/RODframes/'\n","    model_path = '/content/drive/My Drive/Colab Notebooks/RetinaSmartCamera/models/'\n","\n","    #image_path = os.path.join(root_path)\n","    train_path = os.path.join(root_path, 'train/')\n","    test_path = os.path.join(root_path, 'test/')\n","    \n","    master_frames = []\n","    master_labels = []\n","    \n","    frames = []\n","    labels = []\n","\n","    errors = []\n","    min_count = 7200\n","    for index, obj in enumerate(classes):\n","        print('Iterating over {}'.format(obj))\n","        try:\n","            object_frames = []\n","            labels_index = []\n","\n","            object_path = os.path.join(train_path, obj+'/light/original/')\n","            count_index = 0\n","\n","            for frame_index, frame in enumerate(os.listdir(object_path)):\n","                if count_index%skip == 0:\n","                    frame_path = os.path.join(object_path, frame)\n","                    object_frames.append(frame_path)\n","                    labels_index.append(frame_index)\n","                count_index += 1\n","            \n","            if len(object_frames) < min_count:\n","                min_count = len(object_frames)\n","            sorted_object_frames = sorted(object_frames, key=natural_key)\n","            master_frames.append(sorted_object_frames)\n","            master_labels.append(labels_index)\n","            print('Number of images for class ',obj, ': ', len(sorted_object_frames))\n","        \n","        except OSError as err:\n","            print(\"OS error for object {}: {}\".format(obj, err))\n","            errors.append((index, obj))\n","            continue\n","    \n","    for ind, obj in errors:\n","        classes.remove(obj)\n","        del master_frames[ind]\n","        del master_labels[ind]\n","    print(classes)\n","    \n","    for i in range(0, min_count):\n","        for object_index in range(len(classes)):\n","            frames.append(master_frames[object_index][i])\n","            labels.append(master_labels[object_index][i])\n","\n","    dataset = ObjectsDataset(root_path, frames, labels, classes, image_type)\n","\n","    train_size = int (0.8 * len(dataset))\n","    validation_size = int(0.15 * len(dataset))\n","    test_size = len(dataset) - train_size - validation_size\n","    train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size, test_size])\n","\n","    dataset_dict = {'train' : train_dataset, 'validation' : validation_dataset, 'test' : test_dataset}\n","\n","    dataloader = {x : torch.utils.data.DataLoader(dataset_dict[x], batch_size = batch_size, shuffle = shuffle, \n","                                                  num_workers = 0, drop_last=True) for x in ['train', 'test', 'validation']}\n","    \n","    dataset_sizes = {x: len(dataset_dict[x]) for x in ['train', 'test', 'validation']}\n","    print('Dataset size is ', dataset_sizes)\n","\n","    return dataloader\n","    "],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqSUqsfzTYKg","executionInfo":{"status":"ok","timestamp":1604064071865,"user_tz":-60,"elapsed":1304,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}}},"source":["def createLabels(classes, set_, root_dir, light,mode,bg=False):\n","  \n","    labels=[]\n","    frame=[]\n","    lightt=[]\n","    k=0\n","    if bg==True:\n","      k=20\n","    for i in classes:\n","        frames = os.listdir(os.path.join(root_dir, set_, i,light,mode))\n","        for a in frames:\n","            labels.append(k)\n","            frame.append(a)\n","            lightt.append(light)\n","        k+=1\n","    return labels,frame, lightt\n","  \n","def orderFramesSequence(labels,light,c=3):\n","  \n","  frame=[]\n","  lab=[]\n","  lt=[]\n","  rang=[]\n","  label,counts=np.unique(labels,return_counts=True)\n","\n","  for h in range(c):\n","    rang.append(counts.copy())\n","  \n","  for a in label:\n","    \n","    f=counts[a].copy()\n","    for v in range(c):\n","      \n","      rem=(f-v) % c\n","      val=f-rem-v\n","      rang[v][a] = val\n","      \n","      \n","    for i in range(c):\n","      \n","      for x in range(rang[i][a]):\n","        \n","        if ((x+1+i)%c)==(i+1):\n","          frame.append('frame'+str(x+1+i)+'.png')\n","          lab.append(a)\n","          lt.append(light)\n","          \n","        elif c==(i+1):\n","          if ((x+1+i)%c)==0:         \n","            frame.append('frame'+str(x+1+i)+'.png')\n","            lab.append(a)\n","            lt.append(light)\n","        \n","  return frame,lab,lt\n","\n","\n","def random_split(dataset, lengths,indic=True,indices=[]):\n","    \"\"\"\n","    Randomly split a dataset into non-overlapping new datasets of given lengths.\n","\n","    Arguments:\n","        dataset (Dataset): Dataset to be split\n","        lengths (sequence): lengths of splits to be produced\n","    \"\"\"\n","    if sum(lengths) != len(dataset):\n","        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n","    if indic==True:\n","      indices = randperm(sum(lengths))\n","    return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n","\n","class MyDataset(Dataset):\n"," \n","\n","    def __init__(self,classes, labels, light, root_dir,sett,typee,frames,transform=None,c=3,rgb=False):\n","       \n","        self.labels = labels\n","        self.root_dir = root_dir\n","        self.sett=sett\n","        self.transform = transform\n","        self.typee=typee\n","        self.frame=frames\n","        self.classes=classes\n","        self.light=light\n","        self.c=c\n","        self.rgb=rgb\n","\n","    def __len__(self):\n","        return len(self.frame)\n","      \n","\n","    \n","    def __getitem__(self, ind):\n","        img_name = os.path.join(self.root_dir,self.sett, \n","                                self.classes[self.labels[ind]],self.light[ind],self.typee,self.frame[ind])\n","        image = cv2.imread(img_name)\n","        if self.rgb==False:\n","            image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n","        image=image/255\n","        \n","        frame_=self.frame[ind][5::]\n","        ind_=int(frame_.replace('.png',''))+1\n","        images=image\n","        \n","        for g in range(self.c-1):\n","          frame_='frame'+str(ind_+g)+'.png'\n","        \n","          img_name2 = os.path.join(self.root_dir,self.sett, \n","                                  self.classes[self.labels[ind]],self.light[ind],self.typee,frame_)\n","          image2 = cv2.imread(img_name2)\n","          if rgb==False:\n","              image2=cv2.cvtColor(image2,cv2.COLOR_BGR2GRAY)\n","          image2=image2/255\n","          images=np.concatenate((images,image2)) \n","        \n","        label = self.labels[ind]\n","        sample = (images , label)\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","\n","class ToTensor(object):\n","    \n","\n","    def __call__(self, sample):\n","        image, label = sample\n","        \n","        \n","\n","        image=torch.from_numpy(image.astype('float32')).view(-1,257,490)\n","        \n","        label=torch.from_numpy(np.asarray(label).astype('long'))\n","        \n","        return (image , label)\n","\n","\n","def createDataloaders(root_dir, classes, bs, c, set_, type_, testset_, train_ind, val_ratio, color_opponency):\n","  while True:\n","    try:\n","      labels_light, frame_light,lightt = createLabels(classes, 'train', root_dir, 'light',set_+'_'+type_)\n","      #labels_dark, frame_dark, darkk = createLabels(classes, 'train', root_dir, 'dark',set_+'_'+type_)\n","      if c>1:\n","        frame_light, labels_light, lightt = orderFramesSequence(labels_light, 'light',c)\n","        #frame_dark, labels_dark, darkk =orderFramesSequence(labels_dark, 'dark',c)\n","      labels=labels_light#+labels_dark\n","      frame=frame_light#+frame_dark\n","      light=lightt#+darkk\n","      \n","      if color_opponency==False:\n","          train = MyDataset(classes, labels, light, root_dir,'train',set_+'_'+type_,frame, transforms.Compose([ ToTensor()]),c=c)\n","      else:\n","          train = MyDatasetColourOpponency(classes, labels, light, root_dir,'train',set_+'_'+type_,frame, transforms.Compose([ ToTensor()]),c=c)\n","\n","\n","\n","\n","      leng = len(train)\n","      test_len = int(leng * val_ratio)\n","      train_len = leng - test_len\n","      indices=train_ind\n","      val_set, train_set_ = random_split(train, [test_len, train_len],True,torch.from_numpy(indices))\n","\n","\n","  #------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","      train_dl = DataLoader(train_set_, batch_size=bs,shuffle=True, num_workers=15)\n","\n","      val_dl = DataLoader(val_set, batch_size=bs,shuffle=True, num_workers=15)\n","\n","      labels_light, frame_light,lightt = createLabels(classes, 'test', root_dir, 'light',testset_+'_'+type_)\n","      if c>1:\n","         frame_light, labels_light, lightt = orderFramesSequence(labels_light, 'light',c=c)\n","      if testset_=='original':\n","          pass\n","          #labels_dark, frame_dark, darkk = createLabels(classes, 'test', root_dir, 'dark',testset_+'_'+type_)\n","      if c>1:\n","          #frame_dark, labels_dark, darkk =orderFramesSequence(labels_dark, 'dark',c=c)\n","        \n","          labels=labels_light#+labels_dark\n","          frame=frame_light#+frame_dark\n","          light=lightt#+darkk\n","      else:\n","          labels=labels_light\n","          frame=frame_light\n","          light=lightt\n","      if color_opponency==False:\n","          test_set = MyDataset(classes, labels, light, root_dir,'test',testset_+'_'+type_,frame, transforms.Compose([ ToTensor()]),c=c)\n","      else:\n","          test_set = MyDatasetColourOpponency(classes, labels, light, root_dir,'test',testset_+'_'+type_,frame, transforms.Compose([ ToTensor()]),c=c)\n","    \n","      test_dl = DataLoader(test_set, batch_size=bs, num_workers=10,shuffle=False)\n","        \n","    \n","      break\n","    except OSError as err:\n","      print(\"OS error: {0}\".format(err))\n","      continue\n","    \n","  dataloaders = {'train': train_dl, 'val': val_dl, 'test': test_dl}\n","  return dataloaders"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7sVwYWfVLCM","executionInfo":{"status":"ok","timestamp":1604064074705,"user_tz":-60,"elapsed":1948,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}}},"source":["from torch import randperm\n","root_dir = '/content/drive/My Drive/RODframes2/RODframes/'\n","classes = ['bag','beer','book','case','coffee','cup','deodorant','eraser',\n","               'hole','mouse','mug','sleep','speaker','spray','stapler','tape',\n","               'tea','tissues','umbrella','watch']\n","bs=50\n","n_frames=1\n","set_='original'\n","type_='corticalimages'\n","testset_ = 'original'\n","color_opponency=False\n","val_ratio = 0.2\n","train_ind = np.array([])\n","dataloaders = createDataloaders(root_dir, classes, bs, n_frames, set_, type_, testset_, train_ind, val_ratio, color_opponency)\n"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZ1-5vPYNWQG","executionInfo":{"status":"ok","timestamp":1604061375753,"user_tz":-60,"elapsed":993,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}},"outputId":"7f0ac58e-849a-4108-885a-0a16896213c2","colab":{"base_uri":"https://localhost:8080/"}},"source":["dataset = get_dataloader(image_type = '', skip = 10, batch_size=8)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Iterating over bag\n","Number of images for class  bag :  210\n","Iterating over beer\n","Number of images for class  beer :  217\n","Iterating over book\n","Number of images for class  book :  212\n","Iterating over case\n","Number of images for class  case :  197\n","Iterating over coffee\n","Number of images for class  coffee :  209\n","Iterating over cup\n","Number of images for class  cup :  224\n","Iterating over deodorant\n","Number of images for class  deodorant :  194\n","Iterating over eraser\n","Number of images for class  eraser :  212\n","Iterating over hole\n","Number of images for class  hole :  212\n","Iterating over mouse\n","Number of images for class  mouse :  201\n","Iterating over mug\n","Number of images for class  mug :  210\n","Iterating over sleep\n","Number of images for class  sleep :  209\n","Iterating over speaker\n","Number of images for class  speaker :  205\n","Iterating over spray\n","Number of images for class  spray :  204\n","Iterating over stapler\n","Number of images for class  stapler :  208\n","Iterating over tape\n","Number of images for class  tape :  200\n","Iterating over tea\n","Number of images for class  tea :  222\n","Iterating over tissues\n","Number of images for class  tissues :  201\n","Iterating over umbrella\n","Number of images for class  umbrella :  194\n","Iterating over watch\n","Number of images for class  watch :  202\n","[]\n","['bag', 'beer', 'book', 'case', 'coffee', 'cup', 'deodorant', 'eraser', 'hole', 'mouse', 'mug', 'sleep', 'speaker', 'spray', 'stapler', 'tape', 'tea', 'tissues', 'umbrella', 'watch']\n","Dataset size is  {'train': 3104, 'test': 194, 'validation': 582}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ktg3MeHvNZL6","executionInfo":{"status":"ok","timestamp":1604066818365,"user_tz":-60,"elapsed":620,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}}},"source":["class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","        ## encoder layers ##\n","        # conv layer (depth from 3 --> 16), 3x3 kernels\n","        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n","        # conv layer (depth from 16 --> 4), 3x3 kernels\n","        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n","        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n","        self.pool = nn.MaxPool2d(2, 2)\n","        \n","        ## decoder layers ##\n","        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n","        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n","        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n","\n","    def forward(self, x):\n","        ## encode ##\n","        # add hidden layers with relu activation function\n","        # and maxpooling after\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        # add second hidden layer\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)  # compressed representation\n","        \n","        ## decode ##\n","        # add transpose conv layers, with relu activation function\n","        x = F.relu(self.t_conv1(x))\n","        # output layer (with sigmoid for scaling from 0 to 1)\n","        x = self.t_conv2(x)\n","        x = F.sigmoid(x)\n","                \n","        return x\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, kernel_size=(5,5))\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2), return_indices=True)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2), return_indices=True)\n","        self.unconv1 = nn.ConvTranspose2d(6,3,kernel_size=(5,5))\n","        self.maxunpool1 = nn.MaxUnpool2d(kernel_size=(2,2))\n","        self.unmaxunpool2 = nn.MaxUnpool2d(kernel_size=(2,2))\n","        \n","        self.encoder1 = nn.Sequential(\n","            nn.Tanh(),\n","            nn.Conv2d(6, 12,kernel_size=(5,5)),\n","        )\n","        \n","        self.encoder2 = nn.Sequential(\n","            nn.Tanh(),\n","            nn.Conv2d(12, 16, kernel_size=(5,5)),\n","            nn.Tanh()\n","        )\n","        \n","        self.decoder2 = nn.Sequential(\n","            nn.ConvTranspose2d(16, 12, kernel_size=(5,5)),\n","            nn.Tanh()\n","        )\n","        \n","        self.decoder1 = nn.Sequential(\n","            nn.ConvTranspose2d(12,6,kernel_size=(5,5)),\n","            nn.Tanh(),\n","        )\n","        \n","\n","    def forward(self, x):\n","        # Encoder\n","        x = self.conv1(x)\n","        x,indices1 = self.maxpool1(x)\n","        x = self.encoder1(x)\n","        x,indices2 = self.maxpool2(x)\n","        x = self.encoder2(x)\n","        \n","        # Decoder\n","        x = self.decoder2(x)\n","        x = self.unmaxunpool2(x, indices2)\n","        x = self.decoder1(x)\n","        x = self.maxunpool1(x,indices1)\n","        x = self.unconv1(x)\n","        x = nn.Tanh()(x)\n","        return x\n"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKZJe_D7RH8W","executionInfo":{"status":"error","timestamp":1604066862828,"user_tz":-60,"elapsed":42850,"user":{"displayName":"Nicholas Sperry","photoUrl":"","userId":"17478191746282896441"}},"outputId":"79c48f58-e1d6-4ca5-a0c3-087c0bbfd5a6","colab":{"base_uri":"https://localhost:8080/","height":426}},"source":["model = ConvAutoencoder()\n","#model = Autoencoder()\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# specify loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","n_epochs = 20\n","\n","for epoch in range(1, n_epochs+1):\n","    # monitor training loss\n","    train_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    for data, label in dataloaders['train']:\n","       \n","        # _ stands in for labels, here\n","        # no need to flatten images\n","        images = data#, _ = data\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        print(images.shape)\n","        outputs = model(images)\n","        # calculate the loss\n","        loss = criterion(outputs, images)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update running training loss\n","        train_loss += loss.item()*images.size(0)\n","            \n","    # print avg training statistics \n","    train_loss = train_loss/len(train_loader)\n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","        epoch, \n","        train_loss\n","        ))"],"execution_count":67,"outputs":[{"output_type":"stream","text":["torch.Size([50, 1, 257, 490])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-b5b658f4f245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 4"]}]},{"cell_type":"code","metadata":{"id":"oCo4yU45ReR3"},"source":[""],"execution_count":null,"outputs":[]}]}